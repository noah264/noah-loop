# LLM æ¨¡å—

## æ¦‚è¿°

LLM æ¨¡å—æ˜¯ Noah Loop ç³»ç»Ÿçš„å¤§è¯­è¨€æ¨¡å‹æœåŠ¡ï¼Œæä¾›ç»Ÿä¸€çš„å¤§è¯­è¨€æ¨¡å‹æ¥å£å’Œç®¡ç†åŠŸèƒ½ã€‚è¯¥æ¨¡å—æ”¯æŒå¤šç§æ¨¡å‹æä¾›å•†ï¼ŒåŒ…æ‹¬ OpenAIã€Anthropicã€æœ¬åœ°æ¨¡å‹ç­‰ï¼Œä¸ºç³»ç»Ÿå…¶ä»–æ¨¡å—æä¾›å¼ºå¤§çš„è¯­è¨€å¤„ç†èƒ½åŠ›ã€‚

## ä¸»è¦åŠŸèƒ½

- ğŸŒ **å¤šæä¾›å•†æ”¯æŒ**ï¼šOpenAIã€Anthropicã€æœ¬åœ°æ¨¡å‹ã€è‡ªå®šä¹‰æä¾›å•†
- ğŸ¯ **ç»Ÿä¸€æ¥å£**ï¼šæ ‡å‡†åŒ–çš„æ¨¡å‹è°ƒç”¨æ¥å£
- ğŸ“Š **æ¨¡å‹ç®¡ç†**ï¼šæ¨¡å‹æ³¨å†Œã€é…ç½®ã€ç›‘æ§
- ğŸ’° **æˆæœ¬æ§åˆ¶**ï¼šä½¿ç”¨é‡ç»Ÿè®¡å’Œæˆæœ¬è®¡ç®—
- ğŸ”„ **æ™ºèƒ½è·¯ç”±**ï¼šæ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹
- ğŸ“ˆ **æ€§èƒ½ç›‘æ§**ï¼šå“åº”æ—¶é—´ã€æˆåŠŸç‡ç­‰æŒ‡æ ‡

## æ”¯æŒçš„æ¨¡å‹ç±»å‹

### 1. èŠå¤©æ¨¡å‹ (Chat)
- å¯¹è¯å¼äº¤äº’
- æ”¯æŒç³»ç»Ÿæç¤ºå’Œå¤šè½®å¯¹è¯
- é€‚ç”¨äºé—®ç­”ã€å¯¹è¯åœºæ™¯

### 2. è¡¥å…¨æ¨¡å‹ (Completion)
- æ–‡æœ¬è¡¥å…¨å’Œç”Ÿæˆ
- é€‚ç”¨äºåˆ›ä½œã€ä»£ç ç”Ÿæˆ

### 3. åµŒå…¥æ¨¡å‹ (Embedding)
- æ–‡æœ¬å‘é‡åŒ–
- é€‚ç”¨äºæœç´¢ã€ç›¸ä¼¼åº¦è®¡ç®—

### 4. å›¾åƒæ¨¡å‹ (Image)
- å›¾åƒç”Ÿæˆå’Œç†è§£
- æ”¯æŒå¤šæ¨¡æ€äº¤äº’

### 5. éŸ³é¢‘æ¨¡å‹ (Audio)
- è¯­éŸ³è½¬æ–‡å­—ã€æ–‡å­—è½¬è¯­éŸ³
- æ”¯æŒè¯­éŸ³äº¤äº’

## æ”¯æŒçš„æä¾›å•†

### OpenAI
- GPT-3.5/4ç³»åˆ—
- DALL-E
- Whisper
- Text-embedding-ada-002

### Anthropic
- Claudeç³»åˆ—æ¨¡å‹

### æœ¬åœ°æ¨¡å‹
- Ollama
- LLaMA
- è‡ªéƒ¨ç½²æ¨¡å‹

### è‡ªå®šä¹‰æä¾›å•†
- æ”¯æŒæ‰©å±•é›†æˆ

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
cd backend/modules/llm
go mod download
```

### 2. é…ç½®ç¯å¢ƒ

åœ¨ `configs/config.yaml` ä¸­é…ç½® LLM æœåŠ¡ï¼š

```yaml
services:
  llm:
    port: 8082
    database:
      url: "postgres://user:password@localhost/noah_loop?sslmode=disable"

# æä¾›å•†é…ç½®
providers:
  openai:
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
  anthropic:
    api_key: "${ANTHROPIC_API_KEY}"
  local:
    endpoint: "http://localhost:11434"
```

### 3. è®¾ç½®ç¯å¢ƒå˜é‡

```bash
export OPENAI_API_KEY="your-openai-api-key"
export ANTHROPIC_API_KEY="your-anthropic-api-key"
```

### 4. å¯åŠ¨æœåŠ¡

```bash
# å¼€å‘ç¯å¢ƒ
go run cmd/main.go

# ç”Ÿäº§ç¯å¢ƒ
go build -o llm cmd/main.go
./llm
```

### 5. éªŒè¯æœåŠ¡

```bash
curl http://localhost:8082/health
```

## API æ–‡æ¡£

### æ¨¡å‹ç®¡ç†

#### æ³¨å†Œæ¨¡å‹
```http
POST /api/v1/models
Content-Type: application/json

{
  "name": "gpt-3.5-turbo",
  "provider": "openai",
  "type": "chat",
  "version": "0613",
  "description": "GPT-3.5 Turboæ¨¡å‹",
  "max_tokens": 4096,
  "price_per_k": 0.002,
  "config": {
    "temperature": 0.7,
    "top_p": 1.0
  },
  "capabilities": ["chat", "function_calling"]
}
```

#### è·å–æ¨¡å‹åˆ—è¡¨
```http
GET /api/v1/models
```

#### è·å–ç‰¹å®šæ¨¡å‹
```http
GET /api/v1/models/{id}
```

#### æ›´æ–°æ¨¡å‹é…ç½®
```http
PUT /api/v1/models/{id}
Content-Type: application/json

{
  "config": {
    "temperature": 0.5,
    "max_tokens": 2048
  }
}
```

#### æ¿€æ´»/åœç”¨æ¨¡å‹
```http
POST /api/v1/models/{id}/activate
POST /api/v1/models/{id}/deactivate
```

### æ¨¡å‹è°ƒç”¨

#### èŠå¤©è¡¥å…¨
```http
POST /api/v1/chat/completions
Content-Type: application/json

{
  "model": "gpt-3.5-turbo",
  "messages": [
    {
      "role": "system",
      "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹"
    },
    {
      "role": "user", 
      "content": "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"
    }
  ],
  "temperature": 0.7,
  "max_tokens": 1000
}
```

#### æ–‡æœ¬è¡¥å…¨
```http
POST /api/v1/completions
Content-Type: application/json

{
  "model": "text-davinci-003",
  "prompt": "äººå·¥æ™ºèƒ½çš„å‘å±•å†å²",
  "max_tokens": 500,
  "temperature": 0.7
}
```

#### æ–‡æœ¬åµŒå…¥
```http
POST /api/v1/embeddings
Content-Type: application/json

{
  "model": "text-embedding-ada-002",
  "input": [
    "äººå·¥æ™ºèƒ½æŠ€æœ¯",
    "æœºå™¨å­¦ä¹ ç®—æ³•"
  ]
}
```

#### å›¾åƒç”Ÿæˆ
```http
POST /api/v1/images/generations
Content-Type: application/json

{
  "model": "dall-e-3",
  "prompt": "ä¸€åªå¯çˆ±çš„æœºå™¨çŒ«åœ¨å¤ªç©ºä¸­é£è¡Œ",
  "size": "1024x1024",
  "quality": "standard",
  "n": 1
}
```

### è¯·æ±‚ç®¡ç†

#### è·å–è¯·æ±‚å†å²
```http
GET /api/v1/requests
```

#### è·å–ç‰¹å®šè¯·æ±‚è¯¦æƒ…
```http
GET /api/v1/requests/{id}
```

#### è¯·æ±‚ç»Ÿè®¡
```http
GET /api/v1/requests/stats
```

## æ•°æ®æ¨¡å‹

### æ¨¡å‹å®ä½“ (Model)
```go
type Model struct {
    ID           uuid.UUID
    Name         string
    Provider     ModelProvider
    Type         ModelType
    Version      string
    Description  string
    Config       map[string]interface{}
    Capabilities []string
    MaxTokens    int
    PricePerK    float64
    IsActive     bool
}
```

### è¯·æ±‚å®ä½“ (Request)
```go
type Request struct {
    ID           uuid.UUID
    ModelID      uuid.UUID
    UserID       uuid.UUID
    Type         RequestType
    Input        map[string]interface{}
    Output       map[string]interface{}
    TokensUsed   int
    Cost         float64
    Duration     time.Duration
    Status       RequestStatus
    ErrorMessage string
}
```

## æä¾›å•†é…ç½®

### OpenAI æä¾›å•†
```yaml
providers:
  openai:
    api_key: "${OPENAI_API_KEY}"
    organization: "${OPENAI_ORG_ID}"  # å¯é€‰
    base_url: "https://api.openai.com/v1"
    timeout: 30s
    retry_attempts: 3
```

### Anthropic æä¾›å•†
```yaml
providers:
  anthropic:
    api_key: "${ANTHROPIC_API_KEY}"
    base_url: "https://api.anthropic.com"
    timeout: 30s
```

### æœ¬åœ°æ¨¡å‹æä¾›å•†
```yaml
providers:
  local:
    endpoint: "http://localhost:11434"
    timeout: 60s
    models:
      - name: "llama2"
        type: "chat"
        max_tokens: 4096
```

## æ¨¡å‹è·¯ç”±

### æ™ºèƒ½è·¯ç”±é…ç½®
```yaml
routing:
  strategies:
    - name: "cost_optimized"
      rules:
        - if: "tokens < 1000"
          model: "gpt-3.5-turbo"
        - if: "tokens >= 1000"
          model: "claude-2"
    
    - name: "performance_optimized"
      rules:
        - if: "task_type == 'code'"
          model: "gpt-4"
        - if: "task_type == 'creative'"
          model: "claude-2"
```

### è‡ªå®šä¹‰è·¯ç”±ç­–ç•¥
```go
type RouterStrategy interface {
    SelectModel(ctx context.Context, request *ModelRequest) (*Model, error)
}

type CostOptimizedRouter struct {
    models []*Model
}

func (r *CostOptimizedRouter) SelectModel(ctx context.Context, request *ModelRequest) (*Model, error) {
    // å®ç°æˆæœ¬ä¼˜åŒ–é€»è¾‘
    return cheapestSuitableModel, nil
}
```

## ç›‘æ§å’ŒæŒ‡æ ‡

### å…³é”®æŒ‡æ ‡

- **è¯·æ±‚é‡**ï¼šæ¯åˆ†é’Ÿ/å°æ—¶/å¤©çš„è¯·æ±‚æ•°
- **å“åº”æ—¶é—´**ï¼šå¹³å‡å“åº”æ—¶é—´ã€P95ã€P99
- **æˆåŠŸç‡**ï¼šè¯·æ±‚æˆåŠŸç‡
- **Token ä½¿ç”¨é‡**ï¼šè¾“å…¥/è¾“å‡º Token ç»Ÿè®¡
- **æˆæœ¬**ï¼šæŒ‰æ¨¡å‹ã€ç”¨æˆ·çš„æˆæœ¬ç»Ÿè®¡
- **é”™è¯¯ç‡**ï¼šå„ç±»é”™è¯¯çš„å‘ç”Ÿç‡

### æŒ‡æ ‡ç«¯ç‚¹
```http
GET /metrics
```

### ç›‘æ§ä»ªè¡¨æ¿

æ¨èä½¿ç”¨ Grafana + Prometheusï¼š

```yaml
# docker-compose.yml
services:
  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
  
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
```

## æˆæœ¬ç®¡ç†

### æˆæœ¬ç»Ÿè®¡
```http
GET /api/v1/costs/summary
GET /api/v1/costs/by-model
GET /api/v1/costs/by-user
```

### é¢„ç®—æ§åˆ¶
```yaml
budget:
  daily_limit: 100.0    # æ¯æ—¥é¢„ç®—é™åˆ¶ï¼ˆç¾å…ƒï¼‰
  user_limits:
    default: 10.0       # é»˜è®¤ç”¨æˆ·é™åˆ¶
    premium: 50.0       # é«˜çº§ç”¨æˆ·é™åˆ¶
  
alerts:
  - threshold: 0.8      # 80% é¢„ç®—æ—¶è­¦å‘Š
    action: "notify"
  - threshold: 1.0      # 100% é¢„ç®—æ—¶é™åˆ¶
    action: "block"
```

## æ‰©å±•å¼€å‘

### è‡ªå®šä¹‰æä¾›å•†

1. å®ç° `ModelProvider` æ¥å£ï¼š

```go
type CustomProvider struct {
    apiKey  string
    baseURL string
}

func (p *CustomProvider) GetName() string {
    return "custom"
}

func (p *CustomProvider) CreateChatCompletion(ctx context.Context, req *ChatCompletionRequest) (*ChatCompletionResponse, error) {
    // å®ç°èŠå¤©è¡¥å…¨é€»è¾‘
    return response, nil
}

func (p *CustomProvider) CreateCompletion(ctx context.Context, req *CompletionRequest) (*CompletionResponse, error) {
    // å®ç°æ–‡æœ¬è¡¥å…¨é€»è¾‘
    return response, nil
}
```

2. æ³¨å†Œæä¾›å•†ï¼š

```go
// åœ¨åˆå§‹åŒ–æ—¶æ³¨å†Œ
app.LLMService.RegisterProvider("custom", &CustomProvider{
    apiKey:  os.Getenv("CUSTOM_API_KEY"),
    baseURL: "https://api.custom.com",
})
```

### ä¸­é—´ä»¶æ”¯æŒ

```go
type Middleware func(ModelHandler) ModelHandler

// æ—¥å¿—ä¸­é—´ä»¶
func LoggingMiddleware(next ModelHandler) ModelHandler {
    return func(ctx context.Context, req *ModelRequest) (*ModelResponse, error) {
        start := time.Now()
        resp, err := next(ctx, req)
        duration := time.Since(start)
        
        log.Printf("Model request: %s, Duration: %v, Error: %v", 
            req.Model, duration, err)
        
        return resp, err
    }
}

// é™æµä¸­é—´ä»¶
func RateLimitMiddleware(limiter *rate.Limiter) Middleware {
    return func(next ModelHandler) ModelHandler {
        return func(ctx context.Context, req *ModelRequest) (*ModelResponse, error) {
            if !limiter.Allow() {
                return nil, ErrRateLimit
            }
            return next(ctx, req)
        }
    }
}
```

## ç¼“å­˜ç­–ç•¥

### Redis ç¼“å­˜é…ç½®
```yaml
cache:
  redis:
    addr: "localhost:6379"
    password: ""
    db: 0
  
  policies:
    embeddings:
      ttl: 24h
      max_size: 10000
    
    completions:
      ttl: 1h
      max_size: 1000
      enable_compression: true
```

### ç¼“å­˜å®ç°
```go
type CacheConfig struct {
    TTL         time.Duration
    MaxSize     int
    Compression bool
}

func (s *LLMService) GetCachedEmbedding(ctx context.Context, text string) ([]float64, bool) {
    key := fmt.Sprintf("embedding:%s", hash(text))
    return s.cache.Get(key)
}
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **API å¯†é’¥æ— æ•ˆ**
   - æ£€æŸ¥ç¯å¢ƒå˜é‡è®¾ç½®
   - éªŒè¯å¯†é’¥æ ¼å¼å’Œæƒé™

2. **è¯·æ±‚è¶…æ—¶**
   - è°ƒæ•´ timeout é…ç½®
   - æ£€æŸ¥ç½‘ç»œè¿æ¥

3. **é…é¢è¶…é™**
   - æŸ¥çœ‹æä¾›å•†é…é¢çŠ¶æ€
   - å®æ–½è¯·æ±‚é™æµ

4. **æ¨¡å‹ä¸å¯ç”¨**
   - æ£€æŸ¥æ¨¡å‹çŠ¶æ€
   - éªŒè¯æä¾›å•†é…ç½®

### è°ƒè¯•å·¥å…·

```bash
# æ£€æŸ¥æ¨¡å‹çŠ¶æ€
curl http://localhost:8082/api/v1/models

# æµ‹è¯•æ¨¡å‹è°ƒç”¨
curl -X POST http://localhost:8082/api/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"gpt-3.5-turbo","messages":[{"role":"user","content":"Hello"}]}'

# æŸ¥çœ‹è¯·æ±‚æ—¥å¿—
tail -f logs/llm.log
```

## æ€§èƒ½ä¼˜åŒ–

### è¿æ¥æ± ä¼˜åŒ–
```yaml
http_client:
  max_idle_conns: 100
  max_idle_conns_per_host: 10
  idle_conn_timeout: 90s
  timeout: 30s
```

### æ‰¹é‡å¤„ç†
```go
// æ‰¹é‡åµŒå…¥å¤„ç†
func (s *LLMService) BatchEmbedding(ctx context.Context, texts []string, batchSize int) ([][]float64, error) {
    var results [][]float64
    
    for i := 0; i < len(texts); i += batchSize {
        end := i + batchSize
        if end > len(texts) {
            end = len(texts)
        }
        
        batch := texts[i:end]
        embeddings, err := s.CreateEmbeddings(ctx, batch)
        if err != nil {
            return nil, err
        }
        
        results = append(results, embeddings...)
    }
    
    return results, nil
}
```

## å®‰å…¨è€ƒè™‘

### API å¯†é’¥ç®¡ç†
- ä½¿ç”¨ç¯å¢ƒå˜é‡å­˜å‚¨æ•æ„Ÿä¿¡æ¯
- å®šæœŸè½®æ¢ API å¯†é’¥
- å®æ–½æœ€å°æƒé™åŸåˆ™

### è¾“å…¥éªŒè¯
```go
func validateInput(input string) error {
    if len(input) > maxInputLength {
        return ErrInputTooLong
    }
    
    if containsSensitiveData(input) {
        return ErrSensitiveContent
    }
    
    return nil
}
```

### è¾“å‡ºè¿‡æ»¤
```go
func filterOutput(output string) string {
    // è¿‡æ»¤æ•æ„Ÿä¿¡æ¯
    return sanitize(output)
}
```

## ç‰ˆæœ¬å†å²

- **v1.0.0**: åŸºç¡€åŠŸèƒ½å®ç°
- **v1.1.0**: æ·»åŠ  Anthropic æ”¯æŒ
- **v1.2.0**: å®ç°æ™ºèƒ½è·¯ç”±
- **v1.3.0**: å¢åŠ æˆæœ¬æ§åˆ¶
- **v1.4.0**: æ”¯æŒæœ¬åœ°æ¨¡å‹

## è´¡çŒ®æŒ‡å—

1. Fork é¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. æ·»åŠ æµ‹è¯•ç”¨ä¾‹
4. æäº¤æ›´æ”¹
5. åˆ›å»º Pull Request

## è®¸å¯è¯

MIT License
